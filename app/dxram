#!/bin/bash

#
# Copyright (C) 2018 Heinrich-Heine-Universitaet Duesseldorf, Institute of Computer Science, Department Operating Systems
#
# This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>
#

# Include guard
if [ "$__CDEPL_APP_DXRAM_INCLUDED" ]; then
    return
fi

__CDEPL_APP_DXRAM_INCLUDED="1"

readonly __DXRAM_BINARY="bin/dxram"

readonly __DXRAM_LOG_FILE_SUPERPEER_POSTFIX="_superpeer"
readonly __DXRAM_LOG_FILE_PEER_POSTFIX="_peer"

readonly __DXRAM_DEFAULT_S_PORT="22221"
readonly __DXRAM_DEFAULT_P_PORT="22222"
readonly __DXRAM_DEFAULT_NETWORK="eth"
readonly __DXRAM_DEFAULT_MSG_HANDLER="2"
readonly __DXRAM_DEFAULT_P_KVSS="1024"
readonly __DXRAM_DEFAULT_P_BACKUP_SRC="false"
readonly __DXRAM_DEFAULT_P_BACKUP_DST="false"

readonly __DXRAM_PROCESS_IDENTIFIER="dxramdeployscript"
readonly __DXRAM_DEFAULT_STARTUP_CONDITION="!---ooo---!"

readonly __DXRAM_REQUIRED_ENVIRONMENT="java/1.8 gcc/6.1"

__DXRAM_RESOLVED_DEFAULT_VALUES=""

__DXRAM_PATH=""
__DXRAM_CONFIG_PATH=""

__DXRAM_OUT_PATH=""
__DXRAM_OUT_CONF_PATH=""
__DXRAM_OUT_LOG_PATH=""

__DXRAM_ZOOKEEPER_NODE=""
__DXRAM_ZOOKEEPER_PORT=""

__DXRAM_NODE_TYPE=()
__DXRAM_NODE_PORT=()
__DXRAM_NODE_NETWORK=()
__DXRAM_NODE_MSG_HANDLER=()
__DXRAM_NODE_RUN_SUDO=()
__DXRAM_NODE_REMOTE_DEBUG_PORT=()
__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT=()
__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB=()
__DXRAM_NODE_PEER_KVSS=()
__DXRAM_NODE_PEER_BACKUP_SRC=()
__DXRAM_NODE_PEER_BACKUP_DST=()
__DXRAM_NODE_MS_ROLE=()

__DXRAM_NODE_AUTOSTART_APP=()
__DXRAM_NODE_AUTOSTART_ARGS=()
__DXRAM_NODE_AUTOSTART_INIT_ID=()

##
# Initialize and setup the DXRAM environment. This must be called before any
# other function of the dxram module.
#
# $1 path Path to folder containing build output of dxram with bin, config
#    folder etc
# $2 zookeeper_node Node running the zookeeper server necessary to bootstrap
#    DXRAM
# $3 zookeeper_port Port of the zookeper server running on the specified node
##
cdepl_app_dxram_init()
{
	local path=$1
	local zookeeper_node=$2
	local zookeeper_port=$3

	__DXRAM_PATH="$(cdepl_cluster_node_cmd 0 "readlink -f $path")"
	__DXRAM_CONFIG_PATH="${__DXRAM_PATH}/config/dxram.json"

	__DXRAM_OUT_PATH="$(cdepl_run_get_cur_out_path)/dxram"
	__DXRAM_OUT_CONF_PATH="${__DXRAM_OUT_PATH}/conf"
	__DXRAM_OUT_LOG_PATH="${__DXRAM_OUT_PATH}/log"

	__DXRAM_ZOOKEEPER_NODE="$zookeeper_node"
	__DXRAM_ZOOKEEPER_PORT="$zookeeper_port"

	# Check if dxram path is available
	if [ ! "$__DXRAM_PATH" ] || [ "$(cdepl_cluster_file_system_cmd "[ -d $__DXRAM_PATH ] && echo \"1\"")" != "1" ]; then
		util_log_error "[dxram]: Path does not exist ($path), resolved path: $__DXRAM_PATH"
	fi

	__cdepl_app_dxram_check
	__cdepl_app_dxram_check_config

	# Output path setup
	cdepl_cluster_file_system_cmd "mkdir -p $__DXRAM_OUT_CONF_PATH"
	cdepl_cluster_file_system_cmd "mkdir -p $__DXRAM_OUT_LOG_PATH"

	util_log "[dxram] Initialized: $__DXRAM_PATH"
	util_log "[dxram] Output: $__DXRAM_OUT_PATH"
}

##
# Set the node type for a DXRAM instance running on a node.
#
# In order to declare a node an actual DXRAM node/instance, you have yo assign
# a role. There is no default node role applied, otherwise. The node will be
# ignored and not considered a DXRAM node/instance.
#
# $1 node Node to set the DXRAM role for
# $2 type DXRAM node type (S or P) to set for the specified node
##
cdepl_app_dxram_node_type()
{
	local node=$1
	local type=$2

	# No default values. If node type not set explicitly, node is not considered to run a DXRAM instance
	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$type" != "S" ] && [ "$type" != "P" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node type $type for node $node"
	fi

	__DXRAM_NODE_TYPE[$node]="$type"
}

##
# Set the port for a node running a DXRAM instance
#
# $1 node Node of the DXRAM instance
# $2 port Port (ethernet) to set
##
cdepl_app_dxram_node_port()
{
	local node=$1
	local port=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$port" -gt "65536" ] || [ "$port" -lt "0" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid port $port for node $node"
	fi

	__DXRAM_NODE_PORT[$node]="$port"
}

##
# Set the network type of DXRAM instance on the target node
#
# $1 node Node of the DXRAM instance
# $2 network Network type to set (eth, ib)
##
cdepl_app_dxram_node_network()
{
	local node=$1
	local network=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$network" != "eth" ] && [ "$network" != "ib" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid network type $network for node $node"
	fi

	__DXRAM_NODE_NETWORK[$node]="$network"
}

##
# Set the number of message handlers for the network subsystem to use for the
# DXRAM instance running on the target node
#
# $1 node Node of the DXRAM instance
# $2 msg_handler Number of message handlers to use on the specified node
##
cdepl_app_dxram_node_message_handler()
{
	local node=$1
	local msg_handler=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_MSG_HANDLER[$node]="$msg_handler"
}

##
# Run the DXRAM instance with sudo on the target node (e.g. might be necessary
# if using InfiniBand networking)
#
# $1 node Node of the DXRAM instance
##
cdepl_app_dxram_run_as_sudo()
{
	local node=$1

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	if [ "$(cdepl_cluster_allows_sudo)" ]; then
		__DXRAM_NODE_RUN_SUDO[$node]="1"
	else
		util_log_warn "[$node][dxram] Cluster type does not allow running commands as sudo, ignored"
	fi
}

##
# Run the DXRAM instance with parameters to enable remote debugging. Once
# enabled, a string with information how to attach the debugger will be printed
# on deployment
#
# $1 node Node of the DXRAM instance
# $2 port Port to assign to the debugger to use
##
cdepl_app_dxram_remote_debug()
{
	local node=$1
	local port=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]="$port"
}

##
# Run the DXRAM instance with parameters to enable profiling with the Yourkit
# profiler. A string with information how to attach the debugger will be printed
# on deployment
#
# $1 node Node of the DXRAM instance
# $2 port Port for the remote profiler
# $3 agent_lib Path on the target node with the libyjpagent.so lib
##
cdepl_app_dxram_remote_profile_yjp()
{
	local node=$1
	local port=$2
	local agent_lib=$3

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	# Check if the agent lib is available
	if [ "$(cdepl_cluster_node_cmd "$node" "[ -f $agent_lib ] && echo \"1\"")" != "1" ]; then
		util_log_error_and_exit "[$node][dxram] Could not find libyjpagent.so in $agent_lib"
	fi

	__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]="$port"
	__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB[$node]="$agent_lib"
}

##
# Set the key-value store size (in MB) for the target DXRAM instance. The
# instance must be declared as a Peer.
#
# $1 node Node of the DXRAM instance
# $2 kvss Key-value storage size in MB
##
cdepl_app_dxram_peer_kvss()
{
	local node=$1
	local kvss=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_PEER_KVSS[$node]="$kvss"
}

##
# Enables/Disables sending backups from the target DXRam instance. The
# instance must be declared as a Peer.
#
# $1 node Node of the DXRAM instance
# $2 value true or false
##
cdepl_app_dxram_peer_backup_src()
{
	local node=$1
	local value=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_PEER_BACKUP_SRC[$node]="$value"
}

cdepl_app_dxram_master_slave_role()
{
	local node=$1
	local value=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_MS_ROLE[$node]="$value"
}

##
# Enables/Disables receiving backups on the target DXRam instance. The
# instance must be declared as a Peer.
#
# $1 node Node of the DXRAM instance
# $2 value true or false
##
cdepl_app_dxram_peer_backup_dst()
{
	local node=$1
	local value=$2

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_PEER_BACKUP_DST[$node]="$value"
}

cdepl_app_dxram_auto_detect_node_config_params()
{
	local node=$1

	# TODO: auto detect optimal
	# network: if /dev/infiniband -> ib, eth otherwise
	# msg handler count: total core count - 2
	# kvss: 1GB OS, 2GB jvm, remain: kvss

	util_log_error_and_exit "Auto detect node config and params not implemented, yet"
}

##
# Start a DXRAM instance on the target node.
#
# $1 node_range_start Node id to start instance on (or range start)
# $2 node_range_end (optional) If specified, all nodes from the range start to
#    end are started
##
cdepl_app_dxram_start_node()
{
	local node_range_start=$1
	local node_range_end=$2

	# Resolve once before first node is started
	if [ ! "$__DXRAM_RESOLVED_DEFAULT_VALUES" ]; then
		__DXRAM_RESOLVED_DEFAULT_VALUES="1"
		__cdepl_app_dxram_resolve_default_config_values
	fi

	if [ "$node_range_end" ]; then
		local pids=""

		for node in $(seq "$node_range_start" "$node_range_end"); do
			__cdepl_app_dxram_start_node "$node" &
			pids="$pids $!"

			counter=$((counter + 1))

			# Parallel deploy in batches of 10 which is the default limit
			# for parallel connections on ssh servers, stay slightly below 
			# this limit (sometimes, 10 causes errors on multiplexing)
			if [ "$counter" -ge "8" ]; then
				wait $pids
				sleep 1
				pids=""
				counter=0
			fi
		done

		wait $pids
	else
		__cdepl_app_dxram_start_node "$node_range_start"
	fi
}

# Compared to cleanup, this executes a soft shutdown calling a dxram node and
# initiating a clean shutdown
cdepl_app_dxram_shutdown_node()
{
	local node=$1
	local shutdown_type=$2

	# TODO
	util_log_error_and_exit "Soft shutdown not implemented"
}

##
# Wait for a started DXRAM instance to be started. A DXRAM instance is
# considered started once all components and services are initialized
# (signaled by a specific string to be printed).
#
# $1 Target node id (or start node id of range if second parameter provided)
# $2 Optional: Node id range end (including) and first parameter is interpreted
#    as range start (including)
##
cdepl_app_dxram_node_wait_started()
{
	local node_range_start=$1
	local node_range_end=$2

	if [ "$node_range_end" ]; then
		local pids=""

		for node in $(seq "$node_range_start" "$node_range_end"); do
			__cdepl_app_dxram_node_wait_started "$node" &
			pids="$pids $!"

			counter=$((counter + 1))

			# Parallel deploy in batches of 10 which is the default limit
			# for parallel connections on ssh servers, stay slightly below 
			# this limit (sometimes, 10 causes errors on multiplexing)
			if [ "$counter" -ge "8" ]; then
				wait $pids
				sleep 1
				pids=""
				counter=0
			fi
		done

		wait $pids
	else
		__cdepl_app_dxram_node_wait_started "$node_range_start"
	fi
}

##
# Cleanup any still running or remaining/crashed instances on the target node
#
# $1 Target node id (or start node id of range if second parameter provided)
# $2 Optional: Node id range end (including) and first parameter is interpreted
#    as range start (including)
##
cdepl_app_dxram_node_cleanup()
{
	local node_range_start=$1
	local node_range_end=$2

	if [ "$node_range_end" ]; then
		local pids=""

		for node in $(seq "$node_range_start" "$node_range_end"); do
			__cdepl_app_dxram_cleanup "$node" &
			pids="$pids $!"

			counter=$((counter + 1))

			# Parallel deploy in batches of 10 which is the default limit
			# for parallel connections on ssh servers, stay slightly below 
			# this limit (sometimes, 10 causes errors on multiplexing)
			if [ "$counter" -ge "8" ]; then
				wait $pids
				sleep 1
				pids=""
				counter=0
			fi
		done

		wait $pids
	else
		__cdepl_app_dxram_cleanup "$node_range_start"
	fi
}

cdepl_app_dxram_autostart()
{
	local node=$1
	local app_class=$2
	local app_arguments=$3
	local app_init_id=$4

	if [ "$node" -ge "$(cdepl_cluster_get_alloc_node_count)" ]; then
		util_log_error_and_exit "[$node][dxram] Invalid node id $node > $(cdepl_cluster_get_alloc_node_count)"
	fi

	__DXRAM_NODE_AUTOSTART_APP[$node]="$app_class"
	__DXRAM_NODE_AUTOSTART_ARGS[$node]="$app_arguments"
	__DXRAM_NODE_AUTOSTART_INIT_ID[$node]="$app_init_id"
}

#################################################

__cdepl_app_dxram_check()
{
	if [ "$(cdepl_cluster_node_cmd 0 "[ -f ${__DXRAM_PATH}/${__DXRAM_BINARY} ] && echo 1")" != "1" ]; then
		util_log_error "[0][dxram] Could not find dxram executable in $__DXRAM_PATH/bin"
	fi
}

__cdepl_app_dxram_check_config()
{
	# Check if config file is available and create default config
	if [ "$(cdepl_cluster_node_cmd 0 "[ -f $__DXRAM_CONFIG_PATH ] && echo \"1\"")" != "1" ]; then
		util_log "[0][dxram] No config file available, creating default config: $__DXRAM_CONFIG_PATH"

		# Don't run this on the login node (might not have java installed)
		# Use the first actual cluster node instead
		cdepl_cluster_node_cmd 0 "cd $__DXRAM_PATH && DXRAM_OPTS='-Ddxram.config=$__DXRAM_CONFIG_PATH' ./$__DXRAM_BINARY > /dev/null 2>&1" "$__DXRAM_REQUIRED_ENVIRONMENT"

		# Sanity check
		if [ "$(cdepl_cluster_node_cmd 0 "[ -f $__DXRAM_CONFIG_PATH ] && echo \"1\"")" != "1" ]; then
			util_log_error "[0][dxram] Creating config file $__DXRAM_CONFIG_PATH failed"
		fi
	else
		local config_content="$(cdepl_cluster_node_cmd 0 "cat '$__DXRAM_CONFIG_PATH'")"
		# Check if corrupted configuration file
		local component_header=$(echo "$config_content" | grep "m_componentConfigs")
		local service_header=$(echo "$config_content" | grep "m_serviceConfigs")
		if [ "$component_header" = "" ] && [ "$service_header" = "" ] ; then
			util_log "[0][dxram] Configuration file $__DXRAM_CONFIG_PATH corrupted, deleting and creating default"

			# Configuration file seems to be corrupted -> start dxram once to create new configuration
			cdepl_cluster_node_cmd 0 "rm $__DXRAM_CONFIG_PATH && cd $__DXRAM_PATH && DXRAM_OPTS='-Ddxram.config=$__DXRAM_CONFIG_PATH' ./$__DXRAM_BINARY && sync > /dev/null 2>&1" "$__DXRAM_REQUIRED_ENVIRONMENT"

			# Sanity check
			if [ "$(cdepl_cluster_node_cmd 0 "[ -f $__DXRAM_CONFIG_PATH ] && echo \"1\"")" != "1" ]; then
				util_log_error "[0][dxram] Creating config file $__DXRAM_CONFIG_PATH failed"
			fi
		fi
	fi
}

__cdepl_app_dxram_resolve_default_config_values()
{
	local node_count="$(cdepl_cluster_get_alloc_node_count)"

	for i in $(seq 0 $((node_count - 1))); do
		# Only for explicitly set node types = DXRAM instance
		if [ "${__DXRAM_NODE_TYPE[$i]}" = "S" ] && [ "${__DXRAM_NODE_PORT[$i]}" = "" ]; then
			__DXRAM_NODE_PORT[$i]="$__DXRAM_DEFAULT_S_PORT"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ] && [ "${__DXRAM_NODE_PORT[$i]}" = "" ]; then
			__DXRAM_NODE_PORT[$i]="$__DXRAM_DEFAULT_P_PORT"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ] && [ "${__DXRAM_NODE_PEER_BACKUP_SRC[$i]}" = "" ]; then
			__DXRAM_NODE_PEER_BACKUP_SRC[$i]="$__DXRAM_DEFAULT_P_BACKUP_SRC"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ] && [ "${__DXRAM_NODE_PEER_BACKUP_DST[$i]}" = "" ]; then
			__DXRAM_NODE_PEER_BACKUP_DST[$i]="$__DXRAM_DEFAULT_P_BACKUP_DST"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ] && [ "${__DXRAM_NODE_PORT[$i]}" = "" ]; then
			__DXRAM_NODE_PORT[$i]="$__DXRAM_DEFAULT_P_PORT"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" = "P" ] && [ "${__DXRAM_NODE_PEER_KVSS[$i]}" = "" ]; then
			__DXRAM_NODE_PEER_KVSS[$i]="$__DXRAM_DEFAULT_P_KVSS"
		fi

		if [ "${__DXRAM_NODE_TYPE[$i]}" != "" ]; then
			if [ "${__DXRAM_NODE_NETWORK[$i]}" = "" ]; then
				__DXRAM_NODE_NETWORK[$i]="$__DXRAM_DEFAULT_NETWORK"
			fi

			if [ "${__DXRAM_NODE_MSG_HANDLER[$i]}" = "" ]; then
				__DXRAM_NODE_MSG_HANDLER[$i]="$__DXRAM_DEFAULT_MSG_HANDLER"
			fi
		fi
	done
}

__cdepl_app_dxram_create_node_base_config() 
{
	local node=$1
	local node_config_path="${__DXRAM_OUT_CONF_PATH}/node_${node}.conf"
	
	cdepl_cluster_file_system_cmd "cp $__DXRAM_CONFIG_PATH $node_config_path"
}

__cdepl_app_dxram_start_node()
{
	local node=$1

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "" ]; then
		util_log_error "[$node][dxram] No node type set, cannot start instance"
	fi

	local logfile=""

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "S" ]; then
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_SUPERPEER_POSTFIX}
	else
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_PEER_POSTFIX}
	fi

	__cdepl_app_dxram_create_node_base_config $node

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "S" ]; then
		__cdepl_app_dxram_start_superpeer "$node" "$logfile"
	elif [ "${__DXRAM_NODE_TYPE[$node]}" = "P" ]; then
		__cdepl_app_dxram_start_peer "$node" "$logfile"
	fi
}

__cdepl_app_dxram_start_superpeer()
{
	local node=$1
	local logfile=$2

	local node_config_path="${__DXRAM_OUT_CONF_PATH}/node_${node}.conf"

	util_log "[$node][dxram][S] Starting superpeer, logfile: $logfile config: $node_config_path"

	local name_to_id="$(cdepl_cluster_node_resolve_node_to_hostname "$node")"

	local ip="$(cdepl_cluster_resolve_hostname_to_ip "$name_to_id")"

	if [ ! "$ip" ]; then
		util_log_error "[$node][dxram][S] Could not resolve hostname '$name_to_id' to ip"
	fi

	local vm_opts=""

	# Required to fix JNI crashing with libIbdxnet (see JNINotes.md in ibnet repository)
	vm_opts="-XX:+UseMembar"

	vm_opts="$vm_opts -Ddxram.config=$node_config_path"
	vm_opts="$vm_opts -Ddxram.m_engineConfig.m_address.m_ip=$ip"
	vm_opts="$vm_opts -Ddxram.m_engineConfig.m_address.m_port=${__DXRAM_NODE_PORT[$node]}"
	vm_opts="$vm_opts -Ddxram.m_engineConfig.m_role=Superpeer"

	# ZooKeeper connection settings
	local zookeeper_ip="$(cdepl_cluster_resolve_node_to_ip "$__DXRAM_ZOOKEEPER_NODE")"
	vm_opts="$vm_opts -Ddxram.m_componentConfigs[ZookeeperBootComponent].m_connection.m_ip=${zookeeper_ip}"
	vm_opts="$vm_opts -Ddxram.m_componentConfigs[ZookeeperBootComponent].m_connection.m_port=${__DXRAM_ZOOKEEPER_PORT}"
	
	# Optional dxram node specific settings

	if [ "${__DXRAM_NODE_NETWORK[$node]}" = "eth" ]; then
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[NetworkComponent].m_coreConfig.m_device=ethernet"
	elif [ "${__DXRAM_NODE_NETWORK[$node]}" = "ib" ]; then
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[NetworkComponent].m_coreConfig.m_device=infiniband"
	fi

	if [ "${__DXRAM_NODE_MSG_HANDLER[$node]}" ]; then
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[NetworkComponent].m_coreConfig.m_numMessageHandlerThreads=${__DXRAM_NODE_MSG_HANDLER[$node]}"
	fi

	# Development and debugging

	local root=""
	if [ "${__DXRAM_NODE_RUN_SUDO[$node]}" = "1" ]; then
		util_log "[$node][dxram][S] Running with sudo"
		root="sudo -E -P"
	fi

	if [ "${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][S] Enabled remote debugging on port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][S] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}' and connect your debugger to localhost, port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
	fi

	if [ "${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentpath:${__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB[$node]}=port=${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}"

		util_log "[$node][dxram][S] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}' and connect with yourkit using 'Connect to remote application' with the arguments 'localhost:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}'"
	fi

	# Don't use && instead of ;
	# This will hang if ssh with controlmaster and nohup is used
	cdepl_cluster_node_cmd "$node" "cd $__DXRAM_PATH ; JAVA_OPTS='$vm_opts' DXRAM_OPTS='-D${__DXRAM_PROCESS_IDENTIFIER}' $root nohup ./$__DXRAM_BINARY > $logfile 2>&1 &" "$__DXRAM_REQUIRED_ENVIRONMENT"
}

__cdepl_app_dxram_start_peer()
{
	local node=$1
	local logfile=$2

	local node_config_path="${__DXRAM_OUT_CONF_PATH}/node_${node}.conf"

	util_log "[$node][dxram][P] Starting peer, logfile: $logfile config: $node_config_path"

	local name_to_id="$(cdepl_cluster_node_resolve_node_to_hostname "$node")"

	local ip="$(cdepl_cluster_resolve_hostname_to_ip "$name_to_id")"

	if [ ! "$ip" ]; then
		util_log_error "[$node][dxram][P] Could not resolve hostname '$name_to_id' to ip"
	fi

	local vm_opts=""

	# Required to fix JNI crashing with libIbdxnet (see JNINotes.md in ibnet repository)
	vm_opts="-XX:+UseMembar"

	vm_opts="$vm_opts -Ddxram.config=$node_config_path"
	vm_opts="$vm_opts -Ddxram.m_engineConfig.m_address.m_ip=$ip"
	vm_opts="$vm_opts -Ddxram.m_engineConfig.m_address.m_port=${__DXRAM_NODE_PORT[$node]}"
	vm_opts="$vm_opts -Ddxram.m_engineConfig.m_role=Peer"

	# ZooKeeper connection settings
	local zookeeper_ip="$(cdepl_cluster_resolve_node_to_ip "$__DXRAM_ZOOKEEPER_NODE")"
	vm_opts="$vm_opts -Ddxram.m_componentConfigs[ZookeeperBootComponent].m_connection.m_ip=${zookeeper_ip}"
	vm_opts="$vm_opts -Ddxram.m_componentConfigs[ZookeeperBootComponent].m_connection.m_port=${__DXRAM_ZOOKEEPER_PORT}"
	
	# Optional dxram node specific settings

	vm_opts="$vm_opts -Ddxram.m_componentConfigs[BackupComponent].m_backupActive=${__DXRAM_NODE_PEER_BACKUP_SRC[$node]}"
	vm_opts="$vm_opts -Ddxram.m_componentConfigs[BackupComponent].m_availableForBackup=${__DXRAM_NODE_PEER_BACKUP_DST[$node]}"

	if [ "${__DXRAM_NODE_NETWORK[$node]}" = "eth" ]; then
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[NetworkComponent].m_coreConfig.m_device=ethernet"
	elif [ "${__DXRAM_NODE_NETWORK[$node]}" = "ib" ]; then
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[NetworkComponent].m_coreConfig.m_device=infiniband"
	fi

	if [ "${__DXRAM_NODE_MSG_HANDLER[$node]}" ]; then
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[NetworkComponent].m_coreConfig.m_numMessageHandlerThreads=${__DXRAM_NODE_MSG_HANDLER[$node]}"
	fi

	if [ "${__DXRAM_NODE_MS_ROLE[$node]}" ]; then
		vm_opts="$vm_opts -Ddxram.m_serviceConfigs[MasterSlaveComputeServiceConfig].m_role=${__DXRAM_NODE_MS_ROLE[$node]}"
	fi

	if [ "${__DXRAM_NODE_PEER_KVSS[$node]}" ]; then
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[ChunkComponent].m_keyValueStoreSize.m_value=${__DXRAM_NODE_PEER_KVSS[$node]}"
		vm_opts="$vm_opts -Ddxram.m_componentConfigs[ChunkComponent].m_keyValueStoreSize.m_unit=mb"
	fi

	if [ "${__DXRAM_NODE_AUTOSTART_APP[$node]}" != "" ]; then
		vm_opts="$vm_opts -Ddxram.m_serviceConfigs[ApplicationServiceConfig].m_autoStart[0].m_className=${__DXRAM_NODE_AUTOSTART_APP[$node]}"
		vm_opts="$vm_opts -Ddxram.m_serviceConfigs[ApplicationServiceConfig].m_autoStart[0].m_args=${__DXRAM_NODE_AUTOSTART_ARGS[$node]}"
		vm_opts="$vm_opts -Ddxram.m_serviceConfigs[ApplicationServiceConfig].m_autoStart[0].m_startOrderId=${__DXRAM_NODE_AUTOSTART_INIT_ID[$node]}"
		util_log "[$node][dxram][P] Enabled autostart for application ${__DXRAM_NODE_AUTOSTART_APP[$node]}"
	fi

	# Development and debugging

	local root=""
	if [ "${__DXRAM_NODE_RUN_SUDO[$node]}" = "1" ]; then
		util_log "[$node][dxram][P] Running with sudo"
		root="sudo -E -P"
	fi

	if [ "${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][P] Enabled remote debugging on port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
		util_log "[$node][dxram][P] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}' and connect your debugger to localhost, port ${__DXRAM_NODE_REMOTE_DEBUG_PORT[$node]}"
	fi

	if [ "${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}" != "" ]; then
		vm_opts="$vm_opts -agentpath:${__DXRAM_NODE_REMOTE_PROFILE_YJP_AGENT_LIB[$node]}=port=${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}"

		util_log "[$node][dxram][P] On your local machine: establish a tunnel using 'ssh <target_hostname> -L ${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}:<target_hostname>:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}' and connect with yourkit using 'Connect to remote application' with the arguments 'localhost:${__DXRAM_NODE_REMOTE_PROFILE_YJP_PORT[$node]}'"
	fi

	# Don't use && instead of ;
	# This will hang if ssh with controlmaster and nohup is used
	cdepl_cluster_node_cmd "$node" "cd $__DXRAM_PATH ; JAVA_OPTS='$vm_opts' DXRAM_OPTS='-D${__DXRAM_PROCESS_IDENTIFIER}' $root nohup ./$__DXRAM_BINARY > $logfile 2>&1 &" "$__DXRAM_REQUIRED_ENVIRONMENT"
}

__cdepl_app_dxram_node_wait_started()
{
	local node=$1
	local condition=$2

	local type="${__DXRAM_NODE_TYPE[$node]}"
	local logfile=""

	if [ "${__DXRAM_NODE_TYPE[$node]}" = "S" ]; then
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_SUPERPEER_POSTFIX}
	else
		logfile=${__DXRAM_OUT_LOG_PATH}/node${node}${__DXRAM_LOG_FILE_PEER_POSTFIX}
	fi

	# Use default condition if not specfied
	if [ ! "$condition" ]; then
		condition="$__DXRAM_DEFAULT_STARTUP_CONDITION"
	fi

	util_log "[$node][dxram][$type] Waiting for startup: $condition"

	while true; do
		local success=$(cdepl_cluster_node_cmd "$node" "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep '$condition'")
		local fail_init=$(cdepl_cluster_node_cmd "$node" "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep '^Initializing DXRAM failed.$'")
		# Abort execution after an exception was thrown (every exception but NetworkResponseCancelledException)
		local fail_error=$(cdepl_cluster_node_cmd "$node" "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep -i 'exception' | grep -v 'NetworkResponseCancelledException' | grep -v 'fpu_exception'")
		# "A fatal error" -> JVM segfaults
		local fail_error2=$(cdepl_cluster_node_cmd "$node" "cat $logfile 2> /dev/null | sed 's,\x1B\[[0-9;]*[a-zA-Z],,g' | grep -i -e '\[ERROR\]' -e '\# A fatal error'")

		if [ "$success" ]; then
			local pid=$(__cdepl_app_dxram_get_instance_running_pid "$node" "${DXRAM_NODE_PORT[$node]}")

			echo ""

			if [ ! "$pid" ]; then
				util_log_error "[$node][dxram][$type] Could not find started process"
			fi

			util_log "[$node][dxram][$type] Started (pid: $pid)"

			break
		elif [ "$fail_init" ]; then
			echo ""
			util_log_error "[$node][dxram][$type] Could not be started. See log file $logfile"
			return 1
		elif [ "$fail_error" ] || [ "$fail_error2" ]; then
			echo ""
			util_log_error "[$node][dxram][$type] Failed, error or exception. See log file $logfile"
			return 2
		fi

		sleep 1.0
	done

	return 0
}

__cdepl_app_dxram_get_instance_running_pid()
{
	local node=$1
	local port=$2

	if [ "$port" ]; then
		port=".*-Ddxram.m_engineConfig.m_address.m_port=${port}"
	fi

	# Consider port for multiple instances on a single machine (e.g. localhost)
	cdepl_cluster_node_cmd "$node" "pgrep -f 'java.*${__DXRAM_PROCESS_IDENTIFIER}${port}'"
}

__cdepl_app_dxram_cleanup()
{
	local node=$1

	util_log "[$node][dxram] Cleanup..."

	local pid=$(__cdepl_app_dxram_get_instance_running_pid "$node")

	local kill_binary=$(which kill)

	if [ "$pid" ]; then
		# If we or someone else left some garbage processes on the node multiple
		# pids are returned
		for i in $pid; do
			local kill_out=$(cdepl_cluster_node_cmd "$node" "$kill_binary -9 $i 2>&1")

			if [ "$?" = "0" ]; then
				util_log "[$node][dxram] Killed (pid: $i)"
			elif [ "$kill_out" ]; then
				# Probably operation not permitted, try root
				cdepl_cluster_node_cmd "$node" "sudo -P $kill_binary -9 $i > /dev/null 2>&1"

				if [ "$?" = "0" ]; then
					util_log "[$node][dxram] Killed (root) (pid: $i)"
				elif [ "$?" != "1" ]; then
					util_log_warn "[$node][dxram] Killing (root) $i failed, DXRAM instance(s) might stay alive"
				fi
			elif [ "$?" != "1" ]; then
				util_log_warn "[$node][dxram] Killing $i failed, DXRAM instance(s) might stay alive"
			fi
		done
	fi
}
