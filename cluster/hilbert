#!/bin/bash

#
# Copyright (C) 2018 Heinrich-Heine-Universitaet Duesseldorf, Institute of Computer Science, Department Operating Systems
#
# This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>
#

#
# Cluster module for HILBERT HPC system of the Heinrich-Heine-University
# Duesseldorf
#

readonly __HILBERT_MAX_NODES="112"
readonly __HILBERT_HOSTNAME="hpc.rz.uni-duesseldorf.de"

readonly __HILBERT_QSUB="/opt/pbs/bin/qsub"
readonly __HILBERT_QDEL="/opt/pbs/bin/qdel"

readonly __PBS_ATTACH="1"

__HILBERT_USER=""
__HILBERT_APP_INSTALL_DIR=""
__HILBERT_CDEPL_OUT_DIR=""
__HILBERT_PROJECT=""

__HILBERT_TOTAL_NODES="0"

__HILBERT_JOB_ID=""
__HILBERT_JOB_ID_LONG=""
__HILBERT_NODE_MAPPING=()

declare -gA __HILBERT_DEPENDENCY_MAP
__HILBERT_DEPENDENCY_MAP["boost/1.55.0"]="Boost/1.55.0"
__HILBERT_DEPENDENCY_MAP["boost/1.62.0"]="Boost/1.62.0"
__HILBERT_DEPENDENCY_MAP["gcc/4.9.4"]="gcc/4.9.4"
__HILBERT_DEPENDENCY_MAP["gcc/6.1"]="gcc/6.1.0"
__HILBERT_DEPENDENCY_MAP["gcc/6.1.0"]="gcc/6.1.0"
__HILBERT_DEPENDENCY_MAP["java/1.8"]="Java/1.8.0_151"
__HILBERT_DEPENDENCY_MAP["java/1.8.0"]="Java/1.8.0_151"
__HILBERT_DEPENDENCY_MAP["pcre/8.38.0"]="pcre/8.38"
__HILBERT_DEPENDENCY_MAP["protobuf/2.6.1"]="protobuf/2.6.1"

##################################
# "API" impl for applications

cdepl_cluster_get_user()
{
	echo "$__HILBERT_USER"
}

cdepl_cluster_get_alloc_node_count()
{
	echo "$__HILBERT_TOTAL_NODES"
}

cdepl_cluster_application_install_dir()
{
	echo "$__HILBERT_APP_INSTALL_DIR"
}

cdepl_cluster_get_base_path_deploy_out()
{
	echo "$__HILBERT_CDEPL_OUT_DIR"
}

cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

	echo "$(__cdepl_cluster_login_cmd "getent hosts $hostname | sed -E 's/([0-9]*.[0-9]*.[0-9]*.[0-9]*).*/\1/p' | head -n 1")"
}

cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	cdepl_cluster_resolve_hostname_to_ip "${__HILBERT_NODE_MAPPING[$node]}"
}

cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	echo "${__HILBERT_NODE_MAPPING[$node]}"
}

cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"
	local required_env="$3"

	local env=""

	if [ "$required_env" ]; then
		local hilbert_env=""

		for entry in $required_env; do
			local hilbert_dep=${__HILBERT_DEPENDENCY_MAP[$entry]}

			if [ ! "$hilbert_dep" ]; then
				util_log_error_and_exit "[hilbert][$node] Could not resolve environment $entry, no mapping available"
			fi

			hilbert_env="$hilbert_env $hilbert_dep"
		done

		env="module load $hilbert_env ; "
	fi

    local taskset=""
    local inner_command=""
    local complete_command=""
    local ssh_wrapped=""

    printf -v cmd '%q ' $cmd
    # Limit to single socket?
    if [ "${__HILBERT_NODES_SINGLE_SOCKET[$node]}" ]; then
        printf -v inner_command '%q ' $env taskset -c 0-11 bash -c "\"$cmd\""
    else
		printf -v inner_command '%q ' $env bash -c "\"$cmd\""
    fi

    # Run inside pbs_attach for resource monitoring
    if [ "$__PBS_ATTACH" == "1" ]; then
		printf -v complete_command '%q ' pbs_attach -P -s -j $__HILBERT_JOB_ID_LONG bash -c "\"$inner_command\""
	else
		printf -v complete_command '%q 'bash -c "\"$inner_command\""
	fi

	printf -v ssh_wrapped '%q ' ssh -q ${__HILBERT_NODE_MAPPING[$node]} bash -c "\"$complete_command\""

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ServerAliveInterval=60 -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME} -n -f bash -c "\"$ssh_wrapped\""
}

cdepl_cluster_allows_sudo()
{
	echo ""
}

cdepl_cluster_upload_to_remote()
{
	local local_path=$1
	local remote_path=$2
	local recursive=$3

	if [ "$recursive" ]; then
		recursive="-r"
	fi

	util_log_debug "[hilbert] Upload $local_path -> $remote_path"

	# Use ControlMaster to establish the TCP connection, once, and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $local_path ${__HILBERT_USER}@${__HILBERT_HOSTNAME}:${remote_path} > /dev/null 2>&1
}

cdepl_cluster_download_from_remote()
{
	local remote_path=$1
	local local_path=$2
	local recursive=$3

	if [ "$recursive" ]; then
		recursive="-r"
	fi

	util_log_debug "[hilbert] Download $remote_path -> $local_path"

	# Use ControlMaster to establish the TCP connection, once, and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME}:$remote_path $local_path > /dev/null 2>&1
}

cdepl_cluster_file_system_cmd()
{
	local cmd="$1"

	# We use a scratch file system for hilbert -> send command to login node
	__cdepl_cluster_login_cmd "$cmd"
}

##################################
# called by cdepl and commands from cdepl console

_cdepl_cluster_init()
{
	__HILBERT_USER="$(cdepl_configuration_get_and_check "user")"
	__HILBERT_APP_INSTALL_DIR="$(cdepl_configuration_get_and_check "app_install_dir" "/home/$__HILBERT_USER")"
	__HILBERT_CDEPL_OUT_DIR="$(cdepl_configuration_get_and_check "cdepl_out_dir" "/scratch_gs/$__HILBERT_USER")"
	__HILBERT_PROJECT="$(cdepl_configuration_get_and_check "project")"

	# Delete old socket handle which hangs ssh calls after restarting on broken connection
	rm -f ~/.ssh/cdepl_*.sock 

	# Check if passwordless auth available
	ssh -o PasswordAuthentication=no -o BatchMode=yes ${__HILBERT_USER}@${__HILBERT_HOSTNAME} exit &> /dev/null

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hilbert] Can't connect to login node, ensure passwordless auth is set up and the cluster user is valid"
	fi

	# Force create socket for ssh connection
	# There seems to be a weird bug that the first command sent over the newly
	# created socket is silently failing
	# e.g. mkdir doesn't create the directory but doesn't error
    # Execute noop
	__cdepl_cluster_login_cmd ":"
}

_cdepl_cluster_status()
{
	echo "Current jobs running of user $__HILBERT_USER"
	__cdepl_cluster_login_cmd "qstat -u $__HILBERT_USER"

	echo ""

	local resource_available="$(curl -k "https://myjam3.hhu.de/ajax.php?modul=cluster&script=FreeRessources" 2> /dev/null)"
    printf "Resources currently available:\n${resource_available}\n"
}

_cdepl_cluster_alloc()
{
	local arg=$1
	local list=$2 
	local job_args=""

	if [ ! "$arg" ]; then
		util_log "Usage: cluster alloc <num nodes> or cluster alloc list <comma separated list of node nums, e.g. 65,66>"
		return
	fi

	local node_count=""
	local nodes_explicit=""

	if [ "$arg" = "list" ]; then
		local counter=0

		for i in $(echo $list | sed "s/,/ /g"); do
			nodes_explicit="${nodes_explicit}hilbert${i} "
			counter=$((counter + 1))
		done

		__HILBERT_TOTAL_NODES=$counter

		job_args=${@:3}
	else
		node_count="$arg"
		__HILBERT_TOTAL_NODES=$node_count

		job_args=${@:2}
	fi

	# Also required for job system, default walltime for short jobs < 2h
	local walltime="01:59:00"
	local queue=""
	local job_name="cdepl"
	# Hardware specs
	local cpu_count="1"
	local mem_mb="1024"
	# ivybridge = old nodes, skylake = new nodes
	local arch="skylake"

    # Optional arguments, e.g.
    # q=Devel
    # j=bla

    for arg in $job_args; do
        case "${arg:0:2}" in
			"a=") arch="${arg:2}" ;;
		    "c=") cpu_count="${arg:2}" ;;
            "m=") mem_mb="${arg:2}" ;;
			"j=") job_name="${arg:2}" ;;
            "q=") queue="${arg:2}" ;;
			"t=") walltime="${arg:2}" ;;
            *) ;;
        esac
    done

	# create separate folder for hilbert data
	local timestamp="$(date '+%Y-%m-%d_%H-%M-%S-%3N')" 
	local hilbert_data_path="$(cdepl_cluster_out_path)/hilbert_$timestamp"

	__cdepl_cluster_login_cmd "mkdir -p $hilbert_data_path"

	# Create job script to bootstrap the cluster: allocate nodes, get node mappings
	local tmp_job_script="/tmp/cdepl_hilbert_deploy.job"
	local job_script="$hilbert_data_path/deploy.job"
	local node_mappings_path="$hilbert_data_path/node_mappings"

    # We could pass this stuff as parameters to PBS but want them documented
    # with the job script
    local pbs_job_string=""

    if [ "$nodes_explicit" ]; then
        pbs_job_string="#PBS -l nodes=${nodes_explicit}:ncpus=${cpu_count}:mem=${mem_mb}MB"
        util_log_debug "[hilbert] Explicit node allocation: ${nodes_explicit}"
    else
        pbs_job_string="#PBS -l select=${node_count}:ncpus=${cpu_count}:mem=${mem_mb}MB:arch=${arch}
#PBS -l place=scatter"
    fi

    if [ "$queue" ]; then
        pbs_job_string="$pbs_job_string
#PBS -q ${queue}"
        util_log_debug "[hilbert] Non default queue specified: ${queue}"
    fi

    pbs_job_string="$pbs_job_string
#PBS -l walltime=${walltime}
#PBS -r n
#PBS -N $job_name
#PBS -A $__HILBERT_PROJECT
#PBS -e ${hilbert_data_path}/hilbert.stderr
#PBS -o ${hilbert_data_path}/hilbert.stdout"

    # Glue everything together
	printf '%s' "#!/bin/bash

${pbs_job_string}

NODES_COUNT=\"\"
NODES_HOSTNAME=\"\"
NODE_MAPPINGS=\"\"

echo \"Getting reserved nodes...\"

NODES_COUNT=0
for NODE in \$(cat \$PBS_NODEFILE); do
	NODES_HOSTNAME[\$NODES_COUNT]=\$NODE
	NODES_COUNT=\$[NODES_COUNT + 1]
done

echo \"Total node count: \$NODES_COUNT\"

echo \"Resolving node mappings for deployment: \"${hilbert_data_path}\"\"

i=0
while [ \$i -lt \$NODES_COUNT ]; do
	NODE_MAPPINGS=\"\${NODE_MAPPINGS}\${i} \${NODES_HOSTNAME[i]}\\n\"
	i=\$[i + 1]
done

printf \"\$NODE_MAPPINGS\" > \"$node_mappings_path\"
sync

# Keep job running to keep allocated nodes active
run_time=\$(echo \"${walltime}\" | awk -F: '{ print (\$1 * 3600) + (\$2 * 60) + \$3 }')

echo \"Keep job running for \$run_time seconds...\"
sleep \$run_time

echo \"Job finished\"
" > $tmp_job_script

	sync

	scp $tmp_job_script ${__HILBERT_USER}@${__HILBERT_HOSTNAME}:$job_script > /dev/null 2>&1
	__cdepl_cluster_login_cmd "chmod +x $job_script"

    # A little late but print which resources are currently available. That
    # might give the user a hint if the job can start very soon. Maybe he
    # decides to reduce the resource needs as well to get something started
    # quickly

    local resource_available="$(curl -k "https://myjam3.hhu.de/ajax.php?modul=cluster&script=FreeRessources" 2> /dev/null)"

    util_log_debug "[hilbert] Resources currently available:\n${resource_available}"

	# Submit the job script and wait for the node mappings to be written to disk

	local qsub_output=$(__cdepl_cluster_login_cmd "$__HILBERT_QSUB $job_script")

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hilbert] Submitting job script $job_script failed"
	fi

	# Wait for node_mappings file to appear

	__HILBERT_JOB_ID=$(echo $qsub_output | cut -d '.' -f 1)
	__HILBERT_JOB_ID_LONG=$qsub_output

	util_log_debug "[hilbert] Job submitted $job_script $__HILBERT_JOB_ID ($qsub_output), waiting for job to start..."

	local node_mappings=""

	# ... and get the node mappings assigned by the job system

    local start_time=$(date +%s)
    local diff_time=""
    local time_str="00:00:00"

    echo "Wait time elapsed: 00:00:00"

	while true; do
		# Hack to trigger cache flush of gpfs to avoid waiting longer than
		# necessary (approx 3 sec compared to 30 sec)
		__cdepl_cluster_login_cmd "ls ${hilbert_data_path} > /dev/null 2>&1"

		if [ "$(__cdepl_cluster_login_cmd "[ -f ${node_mappings_path} ] && echo 1")" = "1" ]; then
			node_mappings=$(__cdepl_cluster_login_cmd "cat $node_mappings_path")
			
			while read -r line; do
				local id=$(echo "$line" | cut -d ' ' -f 1)
				local hostname=$(echo "$line" | cut -d ' ' -f 2)

				__HILBERT_NODE_MAPPING[$id]="$hostname"
			done <<< "$node_mappings"

			break
		fi

		sleep 1
        diff_time=$(date +%s)
		diff_time=$((diff_time - start_time))
        time_str=$(printf "%02d:%02d:%02d" $((diff_time / 3600)) $(((diff_time / 60) % 60)) $((diff_time % 60)))

        # Overwrite previous time
        echo -en "\e[1A"; echo -e "\e[0K\rWait time elapsed: $time_str"
	done

	echo ""

	util_log_debug "[hilbert] Job started"
}

_cdepl_cluster_nodes()
{
	local mappings=""

	for ((i = 0; i < $__HILBERT_TOTAL_NODES; i++)); do
		mappings="${mappings}$i ${__HILBERT_NODE_MAPPING[$i]}\n"
	done

	printf "Node mappings of allocation:\n$mappings"
}

_cdepl_cluster_free()
{
	local job_id=$1

	if [ ! "$job_id" ]; then
		job_id=$__HILBERT_JOB_ID
	fi

	util_log "[hilbert] Deleting job $job_id"
	__cdepl_cluster_login_cmd "qdel $job_id" > /dev/null 2>&1
}

##################################
# private to this script

__cdepl_cluster_login_cmd()
{
	local cmd="$1"
    local required_env="$2"

    local env=""

    if [ "$required_env" ]; then
        local hilbert_env=""

        for entry in $required_env; do
            local hilbert_dep=${__HILBERT_DEPENDENCY_MAP[$entry]}

            if [ ! "$hilbert_dep" ]; then
                util_log_error_and_exit "[hilbert][$node] Could not resolve environment $entry, no mapping available"
            fi

            hilbert_env="$hilbert_env $hilbert_dep"
        done

        env="module load $hilbert_env ; "
    fi

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds 
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ServerAliveInterval=60 -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 ${__HILBERT_USER}@${__HILBERT_HOSTNAME} -n -f "$env $cmd"
}