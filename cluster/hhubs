#!/bin/bash

#
# Copyright (C) 2018 Heinrich-Heine-Universitaet Duesseldorf, Institute of Computer Science, Department Operating Systems
#
# This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>
#

#
# Cluster module for the private cluster of the operating systems research
# group of the Heinrich-Heine-University Duesseldorf
#

readonly __HHUBS_SOLLIPULLI_IP="134.99.70.210"
readonly __HHUBS_SOLLIPULLI_IP_NODE_PREFIX="10.112.51"

__HHUBS_SOLLIPULLI_USER=""
__HHUBS_SOLLIPULLI_APP_INSTALL_DIR=""
__HHUBS_SOLLIPULLI_CDEPL_OUT_DIR=""
__HHUBS_CONSUL_MODE=""

__HHUBS_TOTAL_NODES=""

__HHUBS_NODE_MAP=()

##################################
# "API" impl for applications

cdepl_cluster_get_user()
{
	echo "$__HHUBS_SOLLIPULLI_USER"
}

cdepl_cluster_get_alloc_node_count()
{
	echo "$__HHUBS_TOTAL_NODES"
}

cdepl_cluster_application_install_dir()
{
	echo "$__HHUBS_SOLLIPULLI_APP_INSTALL_DIR"
}

cdepl_cluster_get_base_path_deploy_out()
{
	echo "$__HHUBS_SOLLIPULLI_CDEPL_OUT_DIR"
}

cdepl_cluster_resolve_hostname_to_ip()
{
	local hostname=$1

	# Static IPs, no need to resolve them dynamically
	local node_id="$(echo "$hostname" | sed 's/node//')"

	if [ ! "$node_id" ]; then
		echo ""
	else
		echo "$__HHUBS_SOLLIPULLI_IP_NODE_PREFIX.$node_id"
	fi
}

cdepl_cluster_resolve_node_to_ip()
{
	local node=$1

	if [ "${__HHUBS_NODE_MAP[$node]}" = "" ]; then
		util_log_error_and_exit "[hhubs][$node] Resolve node to ip failed, remote node does not exist"
	fi

	cdepl_cluster_resolve_hostname_to_ip "${__HHUBS_NODE_MAP[$node]}"
}

cdepl_cluster_node_resolve_node_to_hostname()
{
	local node=$1

	echo "${__HHUBS_NODE_MAP[$node]}"
}

cdepl_cluster_node_cmd()
{
	local node=$1
	local cmd="$2"
	local required_env="$3"

# TODO ##########################################################
	#echo ">>>>>>> TODO consider consul mode"

	if [ "${__HHUBS_NODE_MAP[$node]}" = "" ]; then
		util_log_error_and_exit "[hhubs][$node] Exec node $cmd, node does not exist"
	fi

	__cdepl_cluster_hhubs_remote_node_cmd ${__HHUBS_NODE_MAP[$node]} "$cmd"
}

cdepl_cluster_upload_to_remote()
{
	local local_path=$1
	local remote_path=$2
	local recursive=$3

	util_log_debug "[hhubs] Upload $local_path -> $remote_path"

	__cdepl_cluster_hhubs_upload_to_sollipulli $local_path $remote_path $recursive
}

cdepl_cluster_download_from_remote()
{
	local remote_path=$1
	local local_path=$2
	local recursive=$3

	util_log_debug "[hhubs] Download $remote_path -> $local_path"

	__cdepl_cluster_hhubs_download_from_sollipulli $remote_path $local_path $recursive
}

cdepl_cluster_file_system_cmd()
{
	local cmd="$1"

	# NFS is installed on hhubs -> send command to sollipulli, only
	__cdepl_cluster_hhubs_remote_sollipulli_cmd "$cmd"
}

cdepl_cluster_allows_sudo()
{
	echo "1"
}

cdepl_cluster_get_required_configuration_keys() {
	echo "user"
}

##################################
# called by cdepl and commands from cdepl console

_cdepl_cluster_init()
{
	__HHUBS_SOLLIPULLI_USER=$(cdepl_configuration_get_and_check "user")
	__HHUBS_SOLLIPULLI_APP_INSTALL_DIR=$(cdepl_configuration_get_and_check "app_install_dir" "/home/$__HHUBS_SOLLIPULLI_USER")
	__HHUBS_SOLLIPULLI_CDEPL_OUT_DIR=$(cdepl_configuration_get_and_check "cdepl_out_dir" "/home/$__HHUBS_SOLLIPULLI_USER")
	__HHUBS_CONSUL_MODE=$(cdepl_configuration_get_and_check "consul_mode" "0")

	if [ ! "$__HHUBS_SOLLIPULLI_USER" ]; then
		exit 1
	fi

	# Delete old socket handle which hangs ssh calls after restarting on broken connection
	unlink $(realpath ~/.ssh/cdepl_*.sock)

	__cdepl_cluster_hhubs_sollipulli_check

	# Force create socket for ssh connection
	# There seems to be a weird bug that the first command sent over the newly
	# created socket is silently failing
	# e.g. mkdir doesn't create the directory but doesn't error
    # Execute noop
	__cdepl_cluster_hhubs_remote_sollipulli_cmd ":"

	if [ "$__HHUBS_CONSUL_MODE" = "1" ]; then
		util_log "[hhubs] Running cluster in consul mode instead of ssh"
	fi
}

_cdepl_cluster_status()
{
	local nodes_alloc=$(__cdepl_cluster_hhubs_remote_sollipulli_cmd "node ls-alloc")
	local nodes_free=$(__cdepl_cluster_hhubs_remote_sollipulli_cmd "node ls-free")

	local node_alloc_nums=""
	local node_free_nums=""
	local count_alloc="0"
	local count_free="0"

	for i in $nodes_alloc; do
		node_alloc_nums="$node_alloc_nums $(echo "$i" | sed -r 's/node([0-9]*)/\1/g')"
		count_alloc=$((count_alloc + 1))
	done

	for i in $nodes_free; do
		node_free_nums="$node_free_nums $(echo "$i" | sed -r 's/node([0-9]*)/\1/g')"
		count_free=$((count_free + 1))
	done

	echo "Allocated nodes ($count_alloc):$node_alloc_nums"
	echo "Free nodes ($count_free):$node_free_nums"
}

_cdepl_cluster_alloc()
{
	local arg=$1
	local list=$2

	if [ ! "$arg" ]; then
		util_log "Usage: cluster alloc <num nodes> or cluster alloc list <comma separated list of node nums, e.g. 65,66>"
		return
	fi

	# First, get list of already allocated nodes and fill up if necessary
	local allocd_nodes=$(__cpepl_cluster_get_allocated_nodes)
	local free_nodes=$(__cpepl_cluster_get_free_nodes)

	if [ "$arg" = "list" ]; then
		for i in $(echo $list | sed "s/,/ /g"); do
			local found=""

			for j in $allocd_nodes; do
				if [ "$j" = "$i" ]; then
					found="1"
					break
				fi
			done

			if [ ! "$found" ]; then
				local still_free=""

				# Check if still free
				for j in $free_nodes; do
					if [ "$j" = "$i" ]; then
						still_free="1"
						break
					fi
				done

				if [ "$still_free" ]; then
					__cdepl_cluster_hhubs_remote_sollipulli_cmd "node alloc node${i}"
				else
					util_log_error "Node $i not free"
				fi
			fi
		done
	else
		local counter=0

		for i in $allocd_nodes; do
			if [ "$counter" -ge "$arg" ]; then
				break
			fi

			counter=$((counter + 1))
		done

		if [ "$counter" -lt "$arg" ]; then
			for i in $free_nodes; do
				if [ "$counter" -ge "$arg" ]; then
					break
				fi

				counter=$((counter + 1))

				__cdepl_cluster_hhubs_remote_sollipulli_cmd "node alloc node${i}"
			done
		fi
	fi

	# Create a node map out of all allocated nodes
	allocd_nodes=$(__cpepl_cluster_get_allocated_nodes)

	__HHUBS_NODE_MAP=""
	__HHUBS_TOTAL_NODES="0"

	local node_map_str=""

	for i in $(echo $allocd_nodes | sed "s/,/ /g"); do
		node_map_str="${node_map_str}${__HHUBS_TOTAL_NODES} node${i}\n"
		__HHUBS_NODE_MAP[$__HHUBS_TOTAL_NODES]="node${i}"
		__HHUBS_TOTAL_NODES=$((__HHUBS_TOTAL_NODES + 1))
	done

	# If consul mode, ensure that consul is running on target nodes
	if [ "$__HHUBS_CONSUL_MODE" = "1" ]; then
		log_info "[hhubs] Run consul deamons on target nodes"

		for ((i = 0; i < $__HHUBS_TOTAL_NODES; i++)); do
			# TODO check if deamon running and run it if not
			echo ">>> $i ${__HHUBS_NODE_MAP[$i]}"
		done
	fi

	printf "Node map ($__HHUBS_TOTAL_NODES):\n$node_map_str"
}

_cdepl_cluster_nodes()
{
	local mappings=""

	for ((i = 0; i < $__HHUBS_TOTAL_NODES; i++)); do
		mappings="${mappings}$i ${__HHUBS_NODE_MAP[$i]}\n"
	done

	printf "Node mappings of allocation:\n$mappings"
}

_cdepl_cluster_free()
{
	local arg=$1
	local list=$2

	if [ ! "$arg" ]; then
		util_log "Usage: cluster free <num nodes> or cluster free list <comma separated list of node nums, e.g. 65,66>"
		return
	fi

	if [ "$arg" = "list" ]; then
		for i in $(echo $list | sed "s/,/ /g"); do
			__cdepl_cluster_hhubs_remote_sollipulli_cmd "node free node${i}"
		done
	else
		local allocd_nodes=$(__cpepl_cluster_get_allocated_nodes)
		local counter=0

		for i in $(echo $allocd_nodes | sed "s/,/ /g"); do
			if [ "$counter" -ge "$arg" ]; then
				return
			fi

			counter=$((counter + 1))

			__cdepl_cluster_hhubs_remote_sollipulli_cmd "node free node${i}"
		done
	fi
}

cdepl_cluster_show_logs()
{
	local path="$1"

	ssh -t $__HHUBS_SOLLIPULLI_IP "less +F -R $path/*"
}

##################################
# private to this script

__cpepl_cluster_get_allocated_nodes()
{
	local nodes=$(__cdepl_cluster_hhubs_remote_sollipulli_cmd "node ls-alloc")

	local node_nums=""

	for i in $nodes; do
		node_nums="$node_nums $(echo "$i" | sed -r 's/node([0-9]*)/\1/g')"
	done

	echo "$node_nums"
}

__cpepl_cluster_get_free_nodes()
{
	local nodes=$(__cdepl_cluster_hhubs_remote_sollipulli_cmd "node ls-free")

	local node_nums=""

	for i in $nodes; do
		node_nums="$node_nums $(echo "$i" | sed -r 's/node([0-9]*)/\1/g')"
	done

	echo "$node_nums"
}

__cdepl_cluster_hhubs_remote_node_cmd()
{
	local node=$1
	local cmd="$2"

	local dest=""

	dest="${__HHUBS_SOLLIPULLI_USER}@${__HHUBS_SOLLIPULLI_IP}"

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $dest -n -f "ssh $node \"$cmd\""
}

__cdepl_cluster_hhubs_upload_to_sollipulli()
{
	local local_path=$1
	local remote_path=$2
	local recursive=$3

	local dest=""
	local port=""

	dest="${__HHUBS_SOLLIPULLI_USER}@${__HHUBS_SOLLIPULLI_IP}:${remote_path}"

	if [ "$recursive " ]; then
		recursive="-r"
	fi

	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $local_path $dest > /dev/null 2>&1
}

__cdepl_cluster_hhubs_download_from_sollipulli()
{
	local remote_path=$1
	local local_path=$2
	local recursive=$3

	local dest=""
	local port=""

	dest="${__HHUBS_SOLLIPULLI_USER}@${__HHUBS_SOLLIPULLI_IP}:${remote_path}"

	if [ "$recursive " ]; then
		recursive="-r"
	fi

	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	scp $recursive -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $dest $local_path > /dev/null 2>&1
}

__cdepl_cluster_hhubs_remote_sollipulli_cmd()
{
	local cmd="$1"

	local dest=""

	dest="${__HHUBS_SOLLIPULLI_USER}@${__HHUBS_SOLLIPULLI_IP}"

	# -n -f for nohup
	# Use ControlMaster to establish the TCP connection, once and keep it active
	# until all sessions are closed and no further session was established
	# within 60 seconds
	# Note when using nohup: You must redirect stdin/stdout and not add any
	# command before it using &&, e.g. cd ~ && nohup ...
	# This will hang the session with controlmaster
	ssh -o ControlMaster=auto -o ControlPath=~/.ssh/cdepl_%r@%h:%p.sock -o ControlPersist=60 $dest -n -f "$cmd"
}

__cdepl_cluster_hhubs_sollipulli_check()
{
	output="$(ssh -o ConnectTimeout=3 -o PasswordAuthentication=no -o BatchMode=yes ${__HHUBS_SOLLIPULLI_USER}@${__HHUBS_SOLLIPULLI_IP} exit)"

	if [ "$?" != "0" ]; then
		util_log_error_and_exit "[hhubs] Can't connect to login node, either login node not reachable or publickey auth failed. Ensure passwordless auth is set up and the cluster user is valid. Output from ssh command: $output"
	fi

	util_log_debug "[hhubs] sollipulli available"
}

__cdepl_cluster_hhubs_alloc_hardware_specs()
{
	TODO=""
}